---
title: Data Archiving
---

<head>
    <meta name="title" content="Data archiving | Redpanda Docs"/>
    <meta name="description" content="Set up data archiving on Redpanda."/>
</head>

:::info
This feature requires an [Enterprise license](../../introduction/licenses). To upgrade, contact [Redpanda sales](https://redpanda.com/try-redpanda?section=enterprise-cloud).
:::

Data archiving is one Tiered Storage use case. With data archiving, you enable remote write for simple topic backup to cloud storage. In the event of a data center failure, data corruption, or cluster migration, you can recover your archived data from the cloud back to your cluster. 

You can enable remote read to make data readable from cloud storage, or you can make data available for read operations only when it's in local storage. 

:::note
Redpanda supports data archiving to Amazon S3 and Google Cloud Storage. 
:::

## Configure data archiving

Data archiving requires a Tiered Storage configuration. See [Tiered Storage](../../data-management/tiered-storage) for detailed information about these data archiving steps:

1. Enable remote write at the cluster level or at topic level. 

2. Set `retention.ms` and `retention.bytes` to unlimited to prevent your backup data from purging.
	
3. Optional: Enable remote read.
   
4. To recover a topic from cloud storage, use [remote recovery](../../data-management/tiered-storage/#remote-recovery).

## Cancel and delete archiving jobs

The data archiving process runs on each node. By default, every 10 seconds the archiving process identifies log segments that are ready for archiving. Those log segments are uploaded when they are closed for new batches, because they contain only batches with offsets that are less than the committed offset of the partition. 

Every upload job handles a single partition for which the node is a leader.
When the leadership for the partition changes, the reconciliation process cancels all upload jobs that are no longer running on a leader and starts new upload jobs for new partitions. 

To delete older archived data, you can use the following tags to safely delete both partition manifests and log segments from S3 buckets when they are no longer needed. Itâ€™s recommended that you keep topic manifests to recover the corresponding topic.

Redpanda uploads the following objects to an S3 bucket:
- Partition manifests
- Topic manifests
- Log segments

Partition and topic manifests use the .json extension, and log segments use the .log extension.
- Log paths match the paths in the Redpanda data folder in the format `<random-prefix>/<namespace>/<topic>/<partition-id>_<revision-id>/<logfile-name>.log`. For example, a log segment path looks like `/06160a28/kafka/redpanda-test/0_2/39459-1-v1.log`. The random characters improve performance with S3 data transfers. 
- Partition and topic manifests use prefixes with all symbols set to `0` except the first one. For example, a manifest path looks like `/00000000/meta/kafka/redpanda-test/topic_manifest.json` or `/00000000/meta/kafka/redpanda-test/6_2/manifest.json`.

Redpanda adds the `rp-type` tag to all objects in S3:
- Partition manifests - `rp-type=partition-manifest`
- Log segments - `rp-type=segment`
- Topic manifests - `rp-type=topic-manifest`
